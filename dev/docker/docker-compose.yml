version: '3.7'
services:
  mongo-setup:
    container_name: mongo-setup
    image: mongo:5.0.9-focal
    restart: on-failure
    networks:
      default:
    volumes:
      - ./scripts:/scripts
    entrypoint: [ "/scripts/setup-mongo.sh" ] # Make sure this file exists (see below for the setup.sh)
    depends_on:
      - mongo1
      - mongo2
      - mongo3

  mongo1:
    hostname: mongo1
    container_name: localmongo1
    image: mongo:5.0.9-focal
    expose:
      - 27017
    ports:
      - "27017:27017"
    restart: always
    entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "rs0", "--journal", "--dbpath", "/data/db"]
    volumes:
      - ./volumes/mongo/data1/db:/data/db # This is where your volume will persist. e.g. VOLUME-DIR = ./volumes/mongodb
      - ./volumes/mongo/data1/configdb:/data/configdb
  mongo2:
    hostname: mongo2
    container_name: localmongo2
    image: mongo:5.0.9-focal
    expose:
      - 27017
    ports:
      - "27018:27017"
    restart: always
    entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "rs0", "--journal", "--dbpath", "/data/db"]
    volumes:
      - ./volumes/mongo/data2/db:/data/db # Note the data2, it must be different to the original set.
      - ./volumes/mongo/data2/configdb:/data/configdb
  mongo3:
    hostname: mongo3
    container_name: localmongo3
    image: mongo:5.0.9-focal
    expose:
      - 27017
    ports:
      - "27019:27017"
    restart: always
    entrypoint: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "rs0", "--journal", "--dbpath", "/data/db"]
    volumes:
      - ./volumes/mongo/data3/db:/data/db
      - ./volumes/mongo/data3/configdb:/data/configdb
  redis:
    image: bitnami/redis:7.0-debian-11
    environment:
      REDIS_PASSWORD: password
    ports:
      - '6379:6379'
    volumes:
      - ./volumes/bitnami/redis:/bitnami
  rabbitmq:
    image: rabbitmq:3-management-alpine
    environment:
      RABBITMQ_DEFAULT_USER: user
      RABBITMQ_DEFAULT_PASS: password
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - ./volumes/rabbitmq/data/:/var/lib/rabbitmq/
      - ./volumes/rabbitmq/log/:/var/log/rabbitmq
  kafka1:
    image: confluentinc/cp-kafka:7.3.0
    hostname: kafka1
    container_name: kafka1
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      KAFKA_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,CONTROLLER://kafka1:39092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_NODE_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:39092,2@kafka2:39092,3@kafka3:39092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
    volumes:
      - ./scripts/kafka-workaround.sh:/tmp/kafka-workaround.sh
    command: "bash -c '/tmp/kafka-workaround.sh && /etc/confluent/docker/run'"

  kafka2:
    image: confluentinc/cp-kafka:7.3.0
    hostname: kafka2
    container_name: kafka2
    ports:
      - "9093:9093"
      - "29093:29093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094
      KAFKA_LISTENERS: INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,CONTROLLER://kafka2:39092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_NODE_ID: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:39092,2@kafka2:39092,3@kafka3:39092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
    volumes:
      - ./scripts/kafka-workaround.sh:/tmp/kafka-workaround.sh
    command: "bash -c '/tmp/kafka-workaround.sh && /etc/confluent/docker/run'"

  kafka3:
    image: confluentinc/cp-kafka:7.3.0
    hostname: kafka3
    container_name: kafka3
    ports:
      - "9094:9094"
      - "29094:29094"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094
      KAFKA_LISTENERS: INTERNAL://kafka3:19094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,CONTROLLER://kafka3:39092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_NODE_ID: 3
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka1:39092,2@kafka2:39092,3@kafka3:39092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
    volumes:
      - ./scripts/kafka-workaround.sh:/tmp/kafka-workaround.sh
    command: "bash -c '/tmp/kafka-workaround.sh && /etc/confluent/docker/run'"

  kafka-ui:
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    ports:
      - "3000:8080"
    restart: always
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka1:19092,kafka2:19093,kafka3:19094
